{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca27146c",
   "metadata": {},
   "source": [
    "# Active Semi-Supervised Learning training strategy\n",
    "\n",
    "In this demo we will show how to perform an active semi-supervised learning strategy for the training of a deep learning neural network which aims to perform the GBM segmentation on TEM images.\n",
    "In this demo we will set generic folders for the images and related masks, which mimic as much as possible the dataset used in the published paper.\n",
    "Thus, the proposed code is ready-to-use with your custom dataset and it could be easily adapted also for other segmentation purposes.\n",
    "\n",
    "The model used for the image segmentation is a classical EfficientNet-b3 architecture.\n",
    "The detailed description about the model architecture could be found in the work of Curti et al. [1](TODO).\n",
    "\n",
    "We used Tensorflow library for the model implementation, so be sure to have installed all the required package before the use of this script.\n",
    "\n",
    "First of all, we need to import the required libraries and define the common variables and path for the use of the segmentation model.\n",
    "The below script could be used for **all** the ASSL rounds of training, with minimum chaning in the below global variables.\n",
    "Take care about the code comments for the correct usage of the script in different rounds.\n",
    "You must take care about the folder tree in which the data (images and masks) are stored.\n",
    "In this demo we assume a folder tree described as:\n",
    "\n",
    "```bash\n",
    "data/\n",
    "├── gbm_patches_round0\n",
    "├── gbm_masks_round0\n",
    "├── gbm_images\n",
    "├── gbm_masks\n",
    "├── validation_patches_pred_round0\n",
    "└── validation_masks_pred_round0\n",
    "```\n",
    "\n",
    "where:\n",
    "* `gbm_patches_round0` contains the entire set of available images in the dataset;\n",
    "* `gbm_masks` contains the entire set of **validated** masks in the dataset;\n",
    "* `gbm_images_round0` contains the images to use during the current (round 0) round of ASSL training;\n",
    "* `gbm_masks_round0` contains the masks to use during the current (round 0) round of ASSL training;\n",
    "* `validation_patches_pred_round0` will be filled by the images with the predictions overlayed for the ASSL validation;\n",
    "* `validation_masks_pred_round0` will be filled by the predictions of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa75fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# define the current round number\n",
    "ASSL_ROUND = 0\n",
    "# define the batch-size to use during the training\n",
    "BATCH = 8\n",
    "# define the directory in which the whole DB of images are stored\n",
    "ALL_IMAGE_FOLDER = './data/gbm_patches'\n",
    "# define the directory in which the whole DB of (validated!!) masks are stored\n",
    "ALL_MASKS_FOLDER = './data/gbm_masks'\n",
    "# define the directory in which the images are stored\n",
    "TRAIN_IMAGE_FOLDER = f'./data/gbm_patches_round{ASSL_ROUND:d}'\n",
    "# define the directory in which the masks are stored\n",
    "TRAIN_MASKS_FOLDER = f'./data/gbm_masks_round{ASSL_ROUND:d}'\n",
    "# define the directory in which the predictions will be saved for the validation\n",
    "PRED_IMAGE_FOLDER = f'./data/validation_patches_pred_round{ASSL_ROUND:d}'\n",
    "# define the directory in which the predictions will be saved\n",
    "PRED_MASKS_FOLDER = f'./data/validation_masks_pred_round{ASSL_ROUND:d}'\n",
    "\n",
    "# crete the prediction folder if needed\n",
    "os.makedirs(PRED_IMAGE_FOLDER, exist_ok=True)\n",
    "os.makedirs(PRED_MASKS_FOLDER, exist_ok=True)\n",
    "\n",
    "# define the output weight file for the best model checkpint\n",
    "OUT_WEIGHT_FILE = f'./checkpoints/model_round{ASSL_ROUND:d}.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67f0443",
   "metadata": {},
   "source": [
    "To monitor also the development of the ASSL strategy, we can take a look at the statistics related to the considered data against the totality.\n",
    "In each ASSL round we will split the available round-data into a training set (90%) and a validation set (10%): the training set will be used for the tuning of the model parameters, while the remaining validation set will be used for the monitoring of the model performances in a set of independent images.\n",
    "For sake of completeness, we will check all the relevant stats for the round evaluation with the following log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e702c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "all_files = glob(f'{ALL_IMAGE_FOLDER}/*')\n",
    "print(f'All available images: {len(all_files):d}')\n",
    "round_files = glob(f'{TRAIN_IMAGE_FOLDER}/*')\n",
    "round_perc = len(round_files)/len(all_files)*100\n",
    "print(f'Images used at round {ASSL_ROUND:d}: {len(round_files):d} ({round_perc:.3f}%)')\n",
    "train_perc = round(len(round_files)*.9)\n",
    "test_perc = round(len(round_files)*.1)\n",
    "print(f'Images used as training at round {ASSL_ROUND:d}: {train_perc:d} (90%)')\n",
    "print(f'Images used as test at round {ASSL_ROUND:d}: {test_perc:d} (10%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f8f8e",
   "metadata": {},
   "source": [
    "Since we want to perform also a data-augmentation step during the training, we will use the APIs of the Tensorflow library for the correct management of the images and desired transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a509b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define the data-augmentation parameters\n",
    "augmentation_params = {\n",
    "    'rotation_range':360,       # all possible rotations\n",
    "    'width_shift_range':0.0,    # avoid width shift\n",
    "    'height_shift_range':0.0,   # avoid height shift\n",
    "    'fill_mode':'reflect',      # use reflection to fill the augmented image\n",
    "    'shear_range':0.,           # avoid shear\n",
    "    'zoom_range':0.,            # avoid zoom\n",
    "    'horizontal_flip':True,     # perform horizontal flip of the image\n",
    "    'vertical_flip':True,       # perform vertical flip of the image\n",
    "    'cval':0.,                  # just the constant value for the augmented background\n",
    "    'validation_split':0.1,     # set the validation set as the 10% of the entire set of data\n",
    "}\n",
    "\n",
    "# define the data augmentation models for images and masks\n",
    "# NOTE: both images and masks must be rescaled into [0, 1] range for \n",
    "# the correct use of the segmentation model!\n",
    "image_augmentation = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    **augmentation_params,\n",
    "    rescale = 1./255\n",
    ")\n",
    "masks_augmentation = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    **augmentation_params,\n",
    "    rescale = 1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a20e09",
   "metadata": {},
   "source": [
    "Now we need to define the data-loader strategy for the images/masks.\n",
    "Using the global information about the folder tree and the data-augmentation models, we can use the Tensorflow APIs as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fixed the dimensionality of the input as 256x256\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# define the training parameters for the data loader\n",
    "train_params = {\n",
    "    'target_size':(IMG_SIZE, IMG_SIZE),  # resize shape of the input\n",
    "    'class_mode':'input',  # this is the input of the model\n",
    "    'batch_size':BATCH,    # set the batch size\n",
    "    'shuffle':True,        # enable shuffling of the data\n",
    "    'seed':42,             # fix the random seed for the reproducibility\n",
    "}\n",
    "\n",
    "# define the data loader for the training data (aka images and masks)\n",
    "train_image_generator = image_augmentation.flow_from_directory(\n",
    "    directory=TRAIN_IMAGE_FOLDER,  # set the folder of the images\n",
    "    **train_params,                # set the training parameters\n",
    "    color_mode='grayscale',        # the images are in GRAY fmt\n",
    "    classes=[''],                  # there are no classes\n",
    "    subset='training'              # this subset is the training one (aka the 90% of the data)\n",
    ")\n",
    "train_masks_generator = masks_augmentation.flow_from_directory(\n",
    "    directory=TRAIN_MASKS_FOLDER,  # set the folder of the masks\n",
    "    **train_params,                # set the training parameters\n",
    "    color_mode='grayscale',        # the masks are in binary format\n",
    "    classes=[''],                  # there are no classes\n",
    "    subset='training'              # this subset is the training one (aka the 90% of the data)\n",
    ")\n",
    "\n",
    "# define the data loader for the validation data (aka images and masks)\n",
    "val_image_generator = image_augmentation.flow_from_directory(\n",
    "    directory=TRAIN_IMAGE_FOLDER,  # the validation images belongs to the same folder of the training ones\n",
    "    **train_params,                # set the training parameters\n",
    "    color_mode='grayscale',        # the images are in GRAY fmt\n",
    "    classes=[''],                  # there are no classes\n",
    "    subset='validation'            # this subset is the validation one (aka the 10% of the data)\n",
    ")\n",
    "val_masks_generator = masks_augmentation.flow_from_directory(\n",
    "    directory=TRAIN_MASKS_FOLDER,  # the validation images belongs to the same folder of the training ones\n",
    "    **train_params,                # set the training parameters\n",
    "    color_mode='grayscale',        # the masks are in binary format\n",
    "    classes=[''],                  # there are no classes\n",
    "    subset='validation'            # this subset is the validation one (aka the 10% of the data)\n",
    ")\n",
    "\n",
    "# NOTE: all the data (training and validation) belongs to the same directory\n",
    "# and the internal subdivision is guaranteed by the subset keyword of the\n",
    "# tensorflow function.\n",
    "\n",
    "# Since we want to combine images and masks into a series of pairs, we\n",
    "# can use a pre-processing on the data loader generator to obtain the\n",
    "# correct input for our model\n",
    "\n",
    "from itertools import itemgetter\n",
    "\n",
    "# create training pairs\n",
    "train_generator = zip(map(itemgetter(0), (train_image_generator)), \n",
    "                      map(itemgetter(0), (train_masks_generator))\n",
    "                     )\n",
    "# create validation pairs\n",
    "validation_generator = zip(map(itemgetter(0), (val_image_generator)), \n",
    "                           map(itemgetter(0), (val_masks_generator))\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c361e855",
   "metadata": {},
   "source": [
    "Now we can build the model setting the missing training parameters, i.e. the loss function and the optimization strategy.\n",
    "Since the model architecture is already defined in the deepskin package, we can directly import it and setting the training parameters.\n",
    "\n",
    "**NOTE:** Since we are inside an ASSL training round, the model weights **must** be initialized as random at each round!\n",
    "\n",
    "The definition of the loss function and metrics used in the original model were combinations of native functions.\n",
    "For their implementation we used the code provided by the `segmentation_models` package (ref. [here](https://github.com/qubvel/segmentation_models)).\n",
    "Importing this library, we defined the loss function as combination of Dice score and Binary Focal Loss functions.\n",
    "The monitoring of the model performances is defined using the standard IoU score and the F-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10043463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models as sm\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "# define the model architecture\n",
    "model = sm.models.efficientnetb3(trainable=True)\n",
    "inp = Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n",
    "l1 = Conv2D(3, (1, 1))(inp)\n",
    "\n",
    "out = model(l1)\n",
    "\n",
    "model = Model(inp, out, name=model.name)\n",
    "\n",
    "# define the model optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=1e-4,\n",
    "    beta_1=0.9, beta_2=0.999,\n",
    "    epsilon=1e-7,\n",
    "    amsgrad=False,\n",
    "    name='Adam'\n",
    ")\n",
    "\n",
    "# define the loss function\n",
    "loss = sm.losses.DiceLoss() + (1 * sm.losses.BinaryFocalLoss())\n",
    "\n",
    "# define the metric functions for the model evaluation along\n",
    "# the training epochs\n",
    "iou_score = sm.metrics.IOUScore(threshold=0.5)\n",
    "fscore = sm.metrics.FScore(threshold=0.5)\n",
    "\n",
    "# set the training parameters\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss=loss,\n",
    "    metrics=[iou_score, fscore],\n",
    "    run_eagerly=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d9149c",
   "metadata": {},
   "source": [
    "When everything about the model parameters is decided and fixed, we can start the training step, enabling all the utilities provided by the Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69041fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model.fit(\n",
    "    x=train_generator, y=None, # define the input data generators\n",
    "    batch_size=BATCH,          # set the batch size\n",
    "    epochs=100,                # set the maximum number of epochs to perform \n",
    "    steps_per_epoch=train_image_generator.n // BATCH, # define the number of steps for each \n",
    "                                                      # epoch according to the data generator\n",
    "    callbacks=[\n",
    "                                                      # define the callback for the model checkpoint\n",
    "                                                      # setting the output file in which save the best results\n",
    "                                                      # given by the minimum of loss obtained\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            OUT_WEIGHT_FILE, \n",
    "            save_weights_only=True, \n",
    "            save_best_only=True, \n",
    "            mode='min'\n",
    "        ),\n",
    "                                                      # define the callback for the reduction of learning rate\n",
    "                                                      # when a plateau of performances is achieved\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(),\n",
    "                                                      # define the callback for the early stopping of the training\n",
    "                                                      # if there are no improvements in the validation loss for 50\n",
    "                                                      # epochs\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            min_delta=1e-4, patience=10, \n",
    "            verbose=True,\n",
    "            mode='auto', baseline=None, \n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "    ],\n",
    "    validation_data=validation_generator,             # set the data validation generator\n",
    "    validation_steps=val_image_generator.n // BATCH,  # define the number of steps for each\n",
    "                                                      # validation according to the data generator\n",
    "    initial_epoch=0,                                  # set the initial epoch counter\n",
    "    validation_freq=1,                                # enable the validation at each epoch\n",
    "    max_queue_size=10,                                # queue of data to use\n",
    "    workers=1,                                        # number of threads to use\n",
    "    use_multiprocessing=False,                        # disable multi-processing\n",
    "    shuffle=True,                                     # enable the shuffling of the data at each epoch\n",
    "    verbose=1                                         # set verbosity level of the training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287fef93",
   "metadata": {},
   "source": [
    "At the end of the training the model will achieved the best performances of the current ASSL round.\n",
    "An important step for the monitoring of the performances is the visualization of the obtained results, expressed in terms of metric parameters and loss along the training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9bd0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# define the legends for the plots\n",
    "fig1_lbl = [ mpatches.Patch(facecolor='blue', label='Train Loss', edgecolor='k', linewidth=2),\n",
    "             mpatches.Patch(facecolor='orange', label='Val Loss', edgecolor='k', linewidth=2)\n",
    "           ]\n",
    "\n",
    "fig2_lbl = [ mpatches.Patch(facecolor='blue', label='IoU train score', edgecolor='k', linewidth=2),\n",
    "             mpatches.Patch(facecolor='orange', label='IoU val score', edgecolor='k', linewidth=2)\n",
    "           ]\n",
    "\n",
    "fig3_lbl = [ mpatches.Patch(facecolor='blue', label='F1-score train', edgecolor='k', linewidth=2),\n",
    "             mpatches.Patch(facecolor='orange', label='F1-score val', edgecolor='k', linewidth=2)\n",
    "           ]\n",
    "\n",
    "epochs = np.arange(len(history.history['loss']))\n",
    "\n",
    "with sns.plotting_context('paper', font_scale=2):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(30, 8))\n",
    "    loss = sns.lineplot(x=epochs, y=history.history['loss'],\n",
    "                        markers=True, dashes=False, \n",
    "                        ax=ax1)\n",
    "    val_loss = sns.lineplot(x=epochs, y=history.history['val_loss'],\n",
    "                            markers=True, dashes=False, \n",
    "                            ax=ax1)\n",
    "    ax1.set_ylabel('Mask Loss values')\n",
    "    sns.despine(ax=ax1, offset=10, top=True, right=True, bottom=False, left=False)\n",
    "    ax1.legend(handles=fig1_lbl, loc='upper right')\n",
    "    \n",
    "    \n",
    "    loss = sns.lineplot(x=epochs, y=history.history['iou_score'],\n",
    "                        markers=True, dashes=False, \n",
    "                        ax=ax2)\n",
    "    val_loss = sns.lineplot(x=epochs, y=history.history['val_iou_score'],\n",
    "                            markers=True, dashes=False, \n",
    "                            ax=ax2)\n",
    "    ax2.set_ylabel('IoU loss')\n",
    "    sns.despine(ax=ax2, offset=10, top=True, right=True, bottom=False, left=False)\n",
    "    ax2.legend(handles=fig2_lbl, loc='best')\n",
    "    \n",
    "    loss = sns.lineplot(x=epochs, y=history.history['f1-score'],\n",
    "                        markers=True, dashes=False, \n",
    "                        ax=ax3)\n",
    "    val_loss = sns.lineplot(x=epochs, y=history.history['val_f1-score'],\n",
    "                            markers=True, dashes=False, \n",
    "                            ax=ax3)\n",
    "    ax3.set_ylabel('F1-score loss')\n",
    "    sns.despine(ax=ax3, offset=10, top=True, right=True, bottom=False, left=False)\n",
    "    ax3.legend(handles=fig3_lbl, loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8059298",
   "metadata": {},
   "source": [
    "Now we need to take care about the prediction of the model on the new data, i.e. the data which belongs to the whole dataset.\n",
    "Since we set the restoring of the best model parameters at the end of the training epochs, we can directly apply the model on the new images, sampled on the dataset folder.\n",
    "\n",
    "**NOTE:** since the image needs some pre-processing step, we need to manually apply the required sequence of instruction before inserting it in the model.\n",
    "\n",
    "The correct management of the ASSL training strategy requires the validation of the entire set of images in the dataset at each round of training.\n",
    "Therefore, if you want to check the effectiveness of the ASSL training, the list of files for the model prediction **must** be collected from the `ALL_IMAGE_FOLDER` folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
